import streamlit as st
import os
import requests
from bs4 import BeautifulSoup
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException
import time



from dotenv import load_dotenv
load_dotenv()

# Load the GROQ API Key
groq_api_key = os.getenv("GROQ_API_KEY")

if groq_api_key is None:
    st.error("Groq API Key is missing.")
    st.stop()  # Stop execution if key is missing

# Huggingface embedding
hf_token = os.getenv("HF_TOKEN")
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

llm = ChatGroq(groq_api_key=groq_api_key, model_name="llama3-groq-70b-8192-tool-use-preview")

prompt = ChatPromptTemplate.from_template(
    """
    The user will ask you a question about which course they should take. Based on the user's query, recommend the most relevant free course from the list of free courses available on Analytics Vidhya (from the URL given to you).

    Include the following information in your response:
    - Course name
    - Course description
    - URL of the course

    - Only recommend free courses (do not include blogs or other types of content).
    - Ensure your recommendation is aligned with the user's query.
    - Ensure to give detials of course in a correct and orgranised manner.

    <context>
    {context}
    <context>

    User's Question: {input}
    """
)

# Function to scrape website content



def scrape_website_content(url):
    try:
        # Set up Chrome options
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Optional: run in headless mode
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        # Initialize the Chrome driver using Service
        service = Service(r'C:\Users\piyus\OneDrive\Desktop\chromedriver-win64\chromedriver-win64\chromedriver.exe')

        # Use Service and options to create the WebDriver
        driver = webdriver.Chrome(service=service, options=chrome_options)

        # Access the website
        driver.get(url)
        time.sleep(2)  # Wait for the page to fully load (can be adjusted)

        # Extract text from paragraphs
        paragraphs = driver.find_elements(By.TAG_NAME, 'p')
        text_content = ' '.join([para.text for para in paragraphs])

        driver.quit()  # Close the browser

        return text_content

    except WebDriverException as e:
        print(f"Error fetching website content: {e}")
        return None

# Create vector embeddings from website content
def create_vector_embedding_from_url(url):
    if "vectors" not in st.session_state:
        st.session_state.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # Scrape website and get the text
        website_content = scrape_website_content(url)
        if website_content:
            # Split the content into documents (chunks)
            st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            st.session_state.final_documents = st.session_state.text_splitter.create_documents([website_content])
            
            # Create vector store
            st.session_state.vectors = FAISS.from_documents(st.session_state.final_documents, st.session_state.embeddings)
        else:
            st.error("Failed to retrieve content from the website")

st.title("üåê RAG Website Q&A With Groq And Llama3")
st.write("**Get your perfect course in just few seconds !**")

url_input = st.text_input("Enter a website URL to use for Q&A", value ="https://courses.analyticsvidhya.com/pages/all-free-courses")
user_prompt = st.text_input("Enter your query from the website content", placeholder = "Suggest me a course for learning Python for Data Science")

if st.button("Website Embedding"):
    create_vector_embedding_from_url(url_input)
    st.write("Vector Database is ready")

import time

if user_prompt:
    # Ensure vectors are initialized before using them
    if "vectors" not in st.session_state:
        st.error("Please create vector embeddings first by clicking 'Website Embedding'.")
    else:
        document_chain = create_stuff_documents_chain(llm, prompt)
        retriever = st.session_state.vectors.as_retriever()
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        start = time.time()
        response = retrieval_chain.invoke({'input': user_prompt})
        print(f"Response time: {time.time() - start}")

        if 'answer' in response:
            st.write(response['answer'])

        # Handle context safely
        if 'context' in response:
            with st.expander("Website similarity Search"):
                for i, doc in enumerate(response['context']):
                    st.write(doc.page_content)
                    st.write('------------------------')